## LLMGuard
LLMGuard is an open-source framework developed to equip developers with code solutions that are more secure than the code initially generated by Large Language Models (LLMs). These code suggestions are obtained through the integration of LLMs and static security code analyzers. LLMGuard also periodically measures the security properties of LLMs, providing researchers and practitioners with an updated security comparison of different LLMs in the wild, along with a historical record of previous measurements.

## Components

**Prompt Agent:**
This component is responsible for receiving a prompt and providing other components with the code model's response. 
Upon receipt of a prompt, "Prompt Agent" undertakes the task of engineering a response.
Particularly, it can reformulate a prompt, pass the prompt to code models and collect the response, and forward the result to other components.

**Security Agent:**
This component has a pivotal role to uncover security issues in LLM-generated code.
Precisely, it is entrusted to pass code to static code analysis engines (such as Semgrep and Weggli), and to collect potential security vulnerabilities.

**Benchmark Agent:**
This component is responsible to benchmark the security properties of LLMs on a periodic basis.
It processes the benchmark prompts, provided in JSON format, and iteratively sends each prompt  to ``Prompt Agent''.
In the end, it measures the security properties of LLMs (e.g., measures code security based on the number of CWEs and their severity) and ranks LLMs respectively. It records this information in the database. 

## Environment

**Running LLMGuard**

1. Activate Python Env:
  - Windows: ``.\venv\Script\activate``
  - *nix:    ``source ./venv/bin/activate``

2. Install Dependecies:
   ``pip install -r requirements.txt``
   
4. Run Development Server:
  ``python manage.py runserver 0.0.0.0:8000``

**Running Standalone Analyzer Engine**

1. Activate Python Env:
  - Windows: ``.\venv\Script\activate``
  - *nix:    ``source ./venv/bin/activate``

2. Install Dependecies:
   ``pip install -r requirements.txt``

3. Install ``weggli``
   - Download compatible weggli executable from https://github.com/weggli-rs/weggli
   - Add executable path to PATH environmental variable.
   
5. Run Stand-alone Analyzer Engine: 
   ``python sub_servers.py``

## Usage
1. Create a superuser by running ``python manage.py createsuperuser``
2. Add API key of a supported LLM in ``/admin/`` under **Prompt Dispatcher > LLMs**
3. Navigate to main page of the tool (``http://localhost:8000``)
4. Type a prompt and wait for the response 
