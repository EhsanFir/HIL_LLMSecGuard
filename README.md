## LLMSecGuard 
LLMSecGuard  is an open-source framework developed to equip developers with code solutions that are more secure than the code initially generated by Large Language Models (LLMs). These code suggestions are obtained through the integration of LLMs and static security code analyzers. LLMGuard also periodically measures the security properties of LLMs, providing researchers and practitioners with an updated security comparison of different LLMs in the wild, along with a historical record of previous measurements.

## Components

**Prompt Agent:**
This component is responsible for receiving a prompt and providing other components with the code model's response. 
Upon receipt of a prompt, "Prompt Agent" undertakes the task of engineering a response.
Particularly, it can reformulate a prompt, pass the prompt to code models and collect the response, and forward the result to other components.

**Security Agent:**
This component has a pivotal role to uncover security issues in LLM-generated code.
Precisely, it is entrusted to pass code to static code analysis engines (such as Semgrep and Weggli), and to collect potential security vulnerabilities.

**Benchmark Agent:**
This component is responsible to benchmark the security properties of LLMs on a periodic basis.
It processes the benchmark prompts, provided in JSON format, and iteratively sends each prompt  to ``Prompt Agent''.
In the end, it measures the security properties of LLMs (e.g., measures code security based on the number of CWEs and their severity) and ranks LLMs respectively. It records this information in the database. 

## Environment

**Running LLMSecGuard **

1. Activate Python Env:
  - Windows: ``.\venv\Script\activate``
  - *nix:    ``source ./venv/bin/activate``

2. Install Dependecies:
   ``pip install -r requirements.txt``
   
4. Run Development Server:
  ``python manage.py runserver 0.0.0.0:8000``

**Running Standalone Analyzer Engine**

1. Activate Python Env:
  - Windows: ``.\venv\Script\activate``
  - *nix:    ``source ./venv/bin/activate``

2. Install Dependecies:
   ``pip install -r requirements.txt``

3. Install ``weggli``
   - Download compatible weggli executable from https://github.com/weggli-rs/weggli
   - Add executable path to PATH environmental variable.

## Usage
1. Create a superuser by running ``python manage.py createsuperuser``
2. Run the backend by executing ``python manage.py runserver 0.0.0.0:8000``
3. Add API key of a supported LLM in ``/admin/`` under **Prompt Dispatcher > LLMModels**
4. Run add/run the analyzer server by navigating to ``http://localhost:8000/analyzers/analyzer``
5. Install front-end dependencies by first navigating into the front-end directory ``cd frontend`` and then running ``npm i``
6. Run the frontend development server by running ``npm run serve``
7. Navigate to main page of the tool (``http://localhost:8080`` or the url provided by VueCLI)
8. Type a prompt and wait for the response

## Deployment
1. To Deploy the front-end run ``npm run build`` to create build file.
2. Change the settings of the back-end server by setting ``DEBUG = False`` in ``backend/backend/settings.py``
3. Create a suitable nginx config that maps ``/api/`` to ``http://localhost:8000/api/``
4. Collect Django static for using Django-Admin in production ``python manage.py collectstatic``
5. Run nginx and the backend as a service

## Docker 
Docker Image is still under development. Once released the link will be provided. 
